{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# espnet_onnx demonstration\n",
    "\n",
    "This notebook provides a simple demonstration of how to export your trained model and use it for inference.\n",
    "\n",
    "see also:\n",
    "- ESPnet: https://github.com/espnet/espnet\n",
    "- espnet_onnx: https://github.com/Masao-Someki/espnet_onnx\n",
    "\n",
    "Author: [Masao Someki](https://github.com/Masao-Someki)\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Install Dependency\n",
    "- Export your model\n",
    "- Inference with onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependency\n",
    "To run this demo, you need to install the following packages.\n",
    "- espnet_onnx\n",
    "- torch >= 1.11.0 (already installed in Colab)\n",
    "- espnet\n",
    "- espnet_model_zoo\n",
    "- onnx\n",
    "\n",
    "`torch`, `espnet`, `espnet_model_zoo`, `onnx` is required to run the exportation demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U espnet_onnx espnet espnet_model_zoo onnx --no-cache-dir\n",
    "\n",
    "# in this demo, we need to update scipy to avoid an error\n",
    "!pip install -U scipy numpy==1.23.5 pyworld==0.3.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we need additional dependency `onnxruntime-gpu` to run inference on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export your model\n",
    "\n",
    "## Export model from espnet_model_zoo\n",
    "\n",
    "The easiest way to export a model is to use `espnet_model_zoo`. You can download, unpack, and export the pretrained models with `export_from_pretrained` method.\n",
    "`espnet_onnx` will save the onnx models into cache directory, which is `${HOME}/.cache/espnet_onnx` in default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the model.\n",
    "from espnet_onnx.export import ASRModelExport\n",
    "\n",
    "tag_name = 'pyf98/librispeech_conformer_hop_length160'\n",
    "\n",
    "m = ASRModelExport()\n",
    "m.set_export_config(\n",
    "    max_seq_len=5000,\n",
    ")\n",
    "m.export_from_pretrained(\n",
    "    tag_name\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with onnxruntime\n",
    "Now, let's use the exported models for inference.\n",
    "Please enable the GPU resource to run the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please provide the tag_name to specify exported model.\n",
    "tag_name = 'pyf98/librispeech_conformer_hop_length160'\n",
    "\n",
    "# upload wav file and let's inference!\n",
    "import librosa\n",
    "from google.colab import files\n",
    "\n",
    "wav_file = files.upload()\n",
    "y, sr = librosa.load(list(wav_file.keys())[0], sr=16000)\n",
    "\n",
    "# Use the exported onnx file to inference.\n",
    "from espnet_onnx import Speech2Text\n",
    "\n",
    "speech2text = Speech2Text(tag_name, providers=['CUDAExecutionProvider'])\n",
    "nbest = speech2text(y)\n",
    "print(nbest[0][0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
